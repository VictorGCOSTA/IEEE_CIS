{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.model_selection as model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O algoritmo KNN (k-Nearest Neighbors) é um algoritmo de aprendizado de máquina que classifica ou faz previsões com base na proximidade dos vizinhos mais próximos. Ele armazena um conjunto de pontos de treinamento com seus rótulos correspondentes.\n",
    "\n",
    "#Implementando o KNN usando numpy\n",
    "\n",
    "class KNN:\n",
    "    #Definindo numeros de vizinhos e elementos de treinamento\n",
    "    def __init__(self, k, X_train, y_train):\n",
    "        self.k = k \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    #Calcular a distância euclidiana\n",
    "    def distancia(self, n1, n2):\n",
    "        distance = np.linalg.norm(n1-n2)\n",
    "        return distance\n",
    "    \n",
    "    #Retornar os K vizinhos mais próximos de value\n",
    "    def get_neighbors(self, value):\n",
    "        distances = []\n",
    "        for x in self.X_train:\n",
    "            distances.append(self.distancia(value,x))\n",
    "        distances = np.asarray(distances)\n",
    "        indices = np.argpartition(distances, self.k)\n",
    "        k_first = indices[:self.k]\n",
    "        return k_first\n",
    "    \n",
    "    #Realizando a previsão de um elemento dado o conjunto de treinamento\n",
    "    def predict(self, value):\n",
    "        knn_indices = self.get_neighbors(value)\n",
    "        knn_labels=[]\n",
    "        for i in knn_indices:\n",
    "            knn_labels.append(self.y_train[i])\n",
    "        occurrences = np.bincount(knn_labels)\n",
    "        node=np.argmax(occurrences)\n",
    "        return node\n",
    "    \n",
    "    #Retornar a acurácia\n",
    "    def accuracy(self, y_test, predictions):\n",
    "        correct = y_test == predictions\n",
    "        acc = (np.sum(correct) / y_test.shape[0])*100\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia de 93.33333333333333% (Classe implementada por mim)\n",
      "Acuracia de 93.33333333333333% (Utilizando SKLEARN)\n"
     ]
    }
   ],
   "source": [
    "#Testando classe acima\n",
    "X,y = datasets.load_iris(return_X_y=True) #Carregando o dataset iris\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.3, random_state=13) #Dividindo o dataset em treinamento e teste\n",
    "knn = KNN(10, X_train, y_train) #Instanciando o knn\n",
    "\n",
    "preds=[] #Variável que vai armazenar o resultado da previsão\n",
    "\n",
    "for x in X_test:\n",
    "    preds.append(knn.predict(x)) # Realizando a previsão em cada elemento de X_test e guardando em preds\n",
    "\n",
    "acc = knn.accuracy(y_test, preds) #Comparando resultados\n",
    "print(f\"Acuracia de {acc}% (Classe implementada por mim)\")\n",
    "\n",
    "#Comparando com o modelo do sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "knnSKL = KNeighborsClassifier(n_neighbors=10)\n",
    "knnSKL.fit(X_train, y_train)\n",
    "y_pred = knnSKL.predict(X_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)*100\n",
    "print(f\"Acuracia de {acc}% (Utilizando SKLEARN)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'fixed acidity', 'volatile acidity', 'citric acid',\n",
      "       'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
      "       'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol',\n",
      "       'quality', 'wine_is_red'],\n",
      "      dtype='object')\n",
      "Unnamed: 0              0\n",
      "fixed acidity           0\n",
      "volatile acidity        0\n",
      "citric acid             0\n",
      "residual sugar          0\n",
      "chlorides               0\n",
      "free sulfur dioxide     0\n",
      "total sulfur dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "quality                 0\n",
      "wine_is_red             0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Unnamed: 0            6497 non-null   float64\n",
      " 1   fixed acidity         6497 non-null   float64\n",
      " 2   volatile acidity      6497 non-null   float64\n",
      " 3   citric acid           6497 non-null   float64\n",
      " 4   residual sugar        6497 non-null   float64\n",
      " 5   chlorides             6497 non-null   float64\n",
      " 6   free sulfur dioxide   6497 non-null   float64\n",
      " 7   total sulfur dioxide  6497 non-null   float64\n",
      " 8   density               6497 non-null   float64\n",
      " 9   pH                    6497 non-null   float64\n",
      " 10  sulphates             6497 non-null   float64\n",
      " 11  alcohol               6497 non-null   float64\n",
      " 12  quality               6497 non-null   float64\n",
      " 13  wine_is_red           6497 non-null   float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 710.7 KB\n",
      "None\n",
      "Acuracia de 98.76923076923076%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       974\n",
      "         1.0       0.98      0.98      0.98       326\n",
      "\n",
      "    accuracy                           0.99      1300\n",
      "   macro avg       0.98      0.98      0.98      1300\n",
      "weighted avg       0.99      0.99      0.99      1300\n",
      "\n",
      "[[966   8]\n",
      " [  8 318]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "vinhos_ds  = pd.read_csv(\"./winequality.csv\") #Lendo o arquivo\n",
    "#Normalizando os dados do ds\n",
    "print(vinhos_ds.columns)\n",
    "for c in vinhos_ds.columns:\n",
    "    vinhos_ds[c] = (vinhos_ds [c] - min(vinhos_ds[c]))/(max(vinhos_ds[c]) -min(vinhos_ds[c]))\n",
    "print(vinhos_ds.isnull().sum()) #Vendo há valores nulos\n",
    "print(vinhos_ds.info()) #Vendo se todas as variáveis são numéricas\n",
    "\n",
    "#Definindo X e Y\n",
    "y = vinhos_ds['wine_is_red']\n",
    "X = vinhos_ds.drop('wine_is_red', axis=1)\n",
    "\n",
    "#Separando em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.2, random_state=13)\n",
    "\n",
    "#Aplicando o KNN para a classificação binária\n",
    "knnSKL = KNeighborsClassifier(n_neighbors=10)\n",
    "knnSKL.fit(X_train, y_train)\n",
    "y_pred = knnSKL.predict(X_test)\n",
    "\n",
    "#Vendo a acurácia do modelo\n",
    "acc = metrics.accuracy_score(y_test, y_pred)*100\n",
    "print(f\"Acuracia de {acc}%\")\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia de 92.38461538461539%\n",
      "Quando os dados foram normalizados o modelo apresentou uma acurácia entre 98-99% e quando os dados não foram normalizado a acurácia caiu para 90-93%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       974\n",
      "           1       0.90      0.78      0.84       326\n",
      "\n",
      "    accuracy                           0.92      1300\n",
      "   macro avg       0.92      0.88      0.89      1300\n",
      "weighted avg       0.92      0.92      0.92      1300\n",
      "\n",
      "[[946  28]\n",
      " [ 71 255]]\n"
     ]
    }
   ],
   "source": [
    "#Comparando os resultados no caso dos dados não serem normalizados\n",
    "import pandas as pd\n",
    "vinhos_ds  = pd.read_csv(\"./winequality.csv\") #Lendo o arquivo\n",
    "\n",
    "#Definindo X e Y\n",
    "y = vinhos_ds['wine_is_red']\n",
    "X = vinhos_ds.drop('wine_is_red', axis=1)\n",
    "\n",
    "#Separando em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.2, random_state=13)\n",
    "\n",
    "#Aplicando o KNN para a classificação binária\n",
    "knnSKL = KNeighborsClassifier(n_neighbors=10)\n",
    "knnSKL.fit(X_train, y_train)\n",
    "y_pred = knnSKL.predict(X_test)\n",
    "\n",
    "#Vendo a acurácia do modelo\n",
    "acc = metrics.accuracy_score(y_test, y_pred)*100\n",
    "print(f\"Acuracia de {acc}%\") #Acurácia relativamente menor quando os dados não estão normalizados\n",
    "\n",
    "print(\"Quando os dados foram normalizados o modelo apresentou uma acurácia entre 98-99% e quando os dados não foram normalizado a acurácia caiu para 90-93%\")\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia de 94.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vh_co\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Verificando a classificação binária usando uma regressão logística\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vinhos_ds  = pd.read_csv(\"./winequality.csv\") #Lendo o arquivo\n",
    "\n",
    "#Definindo X e Y\n",
    "y = vinhos_ds['wine_is_red']\n",
    "X = vinhos_ds.drop('wine_is_red', axis=1)\n",
    "\n",
    "#Separando em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.2, random_state=13)\n",
    "\n",
    "#Aplicando a regressão logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Verificando a acurácia quando se aplica a regressão logística e o dataset não está normalizado\n",
    "acc = metrics.accuracy_score(y_test, y_pred)*100\n",
    "print(f\"Acuracia de {acc}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia de 98.92307692307692%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       974\n",
      "         1.0       0.99      0.97      0.98       326\n",
      "\n",
      "    accuracy                           0.99      1300\n",
      "   macro avg       0.99      0.98      0.99      1300\n",
      "weighted avg       0.99      0.99      0.99      1300\n",
      "\n",
      "Novamente a acurácia foi maior nos modelos que recebiam os dados normalizados\n",
      "Utilizando a regressão logística obtive uma acurácia maior do que com o KNN\n"
     ]
    }
   ],
   "source": [
    "#Verificando a classificação binária usando uma regressão logística em um dataset normalizado\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vinhos_ds  = pd.read_csv(\"./winequality.csv\") #Lendo o arquivo\n",
    "for c in vinhos_ds.columns:\n",
    "    vinhos_ds[c] = (vinhos_ds [c] - min(vinhos_ds[c]))/(max(vinhos_ds[c]) -min(vinhos_ds[c]))\n",
    "\n",
    "#Definindo X e Y\n",
    "y = vinhos_ds['wine_is_red']\n",
    "X = vinhos_ds.drop('wine_is_red', axis=1)\n",
    "\n",
    "#Separando em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.2, random_state=13)\n",
    "\n",
    "#Aplicando a regressão logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Verificando a acurácia quando se aplica a regressão logística e o dataset não está normalizado\n",
    "acc = metrics.accuracy_score(y_test, y_pred)*100\n",
    "print(f\"Acuracia de {acc}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Novamente a acurácia foi maior nos modelos que recebiam os dados normalizados\")\n",
    "print(\"Utilizando a regressão logística obtive uma acurácia maior do que com o KNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vinhos tintos = 1599 / Vinhos brancos = 4898\n",
      "(4898, 14)\n",
      "(1599, 14)\n",
      "(4898, 14)\n",
      "Vinhos tintos = 4898 / Vinhos brancos = 4898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98       969\n",
      "         1.0       0.98      0.98      0.98       991\n",
      "\n",
      "    accuracy                           0.98      1960\n",
      "   macro avg       0.98      0.98      0.98      1960\n",
      "weighted avg       0.98      0.98      0.98      1960\n",
      "\n",
      "[[954  15]\n",
      " [ 17 974]]\n"
     ]
    }
   ],
   "source": [
    "vinhos_ds  = pd.read_csv(\"./winequality.csv\") #Lendo o arquivo\n",
    "print(f\"Vinhos tintos = {vinhos_ds['wine_is_red'].sum()} / Vinhos brancos = {len(vinhos_ds['wine_is_red']) - vinhos_ds['wine_is_red'].sum()}\")\n",
    "#Dataset desbalanceado\n",
    "#Aplicando oversample\n",
    "white = vinhos_ds[vinhos_ds['wine_is_red'] == 0 ]\n",
    "red = vinhos_ds[vinhos_ds['wine_is_red'] == 1 ]\n",
    "print(white.shape)\n",
    "print(red.shape)\n",
    "df_red_over = red.sample(4898, replace=True) #Duplicando dados\n",
    "print(df_red_over.shape)\n",
    "new_vinhos_ds = pd.concat([white, df_red_over], axis=0)\n",
    "#Verificando o novo dataframe\n",
    "print(f\"Vinhos tintos = {new_vinhos_ds['wine_is_red'].sum()} / Vinhos brancos = {len(new_vinhos_ds['wine_is_red']) - new_vinhos_ds['wine_is_red'].sum()}\")\n",
    "#Aplicando a regressão logística no novo data frame\n",
    "for c in new_vinhos_ds.columns:\n",
    "    new_vinhos_ds[c] = (new_vinhos_ds [c] - min(new_vinhos_ds[c]))/(max(new_vinhos_ds[c]) -min(new_vinhos_ds[c]))\n",
    "\n",
    "#Definindo X e Y\n",
    "y = new_vinhos_ds['wine_is_red']\n",
    "X = new_vinhos_ds.drop('wine_is_red', axis=1)\n",
    "\n",
    "#Separando em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.2, random_state=13)\n",
    "\n",
    "#Aplicando a regressão logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Verificando resultados do modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 14)\n",
      "Vinhos tintos = 1599 / Vinhos brancos = 1599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       312\n",
      "         1.0       1.00      0.98      0.99       328\n",
      "\n",
      "    accuracy                           0.99       640\n",
      "   macro avg       0.99      0.99      0.99       640\n",
      "weighted avg       0.99      0.99      0.99       640\n",
      "\n",
      "[[311   1]\n",
      " [  5 323]]\n"
     ]
    }
   ],
   "source": [
    "vinhos_ds  = pd.read_csv(\"./winequality.csv\") #Lendo o arquivo\n",
    "white = vinhos_ds[vinhos_ds['wine_is_red'] == 0 ]\n",
    "red = vinhos_ds[vinhos_ds['wine_is_red'] == 1 ]\n",
    "df_white_under = white.sample(1599, replace=True) #Duplicando dados\n",
    "print(df_white_under.shape)\n",
    "new_vinhos_ds = pd.concat([red, df_white_under], axis=0)\n",
    "#Verificando o novo dataframe\n",
    "print(f\"Vinhos tintos = {new_vinhos_ds['wine_is_red'].sum()} / Vinhos brancos = {len(new_vinhos_ds['wine_is_red']) - new_vinhos_ds['wine_is_red'].sum()}\")\n",
    "#Aplicando a regressão logística no novo data frame\n",
    "for c in new_vinhos_ds.columns:\n",
    "    new_vinhos_ds[c] = (new_vinhos_ds [c] - min(new_vinhos_ds[c]))/(max(new_vinhos_ds[c]) -min(new_vinhos_ds[c]))\n",
    "\n",
    "#Definindo X e Y\n",
    "y = new_vinhos_ds['wine_is_red']\n",
    "X = new_vinhos_ds.drop('wine_is_red', axis=1)\n",
    "\n",
    "#Separando em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.2, random_state=13)\n",
    "\n",
    "#Aplicando a regressão logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Verificando resultados do modelo\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
      "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
      "       'pH', 'sulphates', 'alcohol', 'quality', 'wine_is_red'],\n",
      "      dtype='object')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.73      0.17      0.28        47\n",
      "           5       0.72      0.76      0.74       407\n",
      "           6       0.69      0.79      0.74       592\n",
      "           7       0.72      0.54      0.62       215\n",
      "           8       0.80      0.47      0.59        34\n",
      "           9       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.70      1300\n",
      "   macro avg       0.52      0.39      0.42      1300\n",
      "weighted avg       0.71      0.70      0.69      1300\n",
      "\n",
      "[[  0   0   3   0   0   0   0]\n",
      " [  1   8  24  13   1   0   0]\n",
      " [  0   3 309  93   2   0   0]\n",
      " [  0   0  91 466  35   0   0]\n",
      " [  0   0   4  90 117   4   0]\n",
      " [  0   0   0  11   7  16   0]\n",
      " [  0   0   0   1   1   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vh_co\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\vh_co\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\vh_co\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "vinhos_ds  = pd.read_csv(\"./winequality.csv\") #Lendo o arquivo\n",
    "\n",
    "vinhos_ds = vinhos_ds.drop('Unnamed: 0', axis=1)\n",
    "print(vinhos_ds.columns) #Verificando as colunas\n",
    "X = vinhos_ds.drop('quality', axis=1)\n",
    "y = vinhos_ds['quality']\n",
    "#Dividinho em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Aplicando no modelo\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "#Verificando os resultados\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality\n",
      "6    2836\n",
      "5    2138\n",
      "7    1079\n",
      "4     216\n",
      "8     193\n",
      "3      30\n",
      "9       5\n",
      "Name: count, dtype: int64\n",
      "quality\n",
      "6    2836\n",
      "5    2836\n",
      "7    2836\n",
      "4    2836\n",
      "8    2836\n",
      "3    2836\n",
      "9    2836\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      1.00      1.00       598\n",
      "           4       0.99      1.00      0.99       574\n",
      "           5       0.87      0.86      0.87       574\n",
      "           6       0.82      0.77      0.79       544\n",
      "           7       0.91      0.95      0.93       578\n",
      "           8       1.00      1.00      1.00       576\n",
      "           9       1.00      1.00      1.00       527\n",
      "\n",
      "    accuracy                           0.94      3971\n",
      "   macro avg       0.94      0.94      0.94      3971\n",
      "weighted avg       0.94      0.94      0.94      3971\n",
      "\n",
      "É possível observar que todas as métricas de avalização melhoraram após a aplicação do oversampling\n"
     ]
    }
   ],
   "source": [
    "print(vinhos_ds.quality.value_counts())\n",
    "#É possível observar que os dados não estão balanceados\n",
    "q6 = vinhos_ds[vinhos_ds['quality'] == 6 ]\n",
    "q5 = vinhos_ds[vinhos_ds['quality'] == 5 ]\n",
    "q7 = vinhos_ds[vinhos_ds['quality'] == 7 ]\n",
    "q4 = vinhos_ds[vinhos_ds['quality'] == 4 ]\n",
    "q8 = vinhos_ds[vinhos_ds['quality'] == 8 ]\n",
    "q3 = vinhos_ds[vinhos_ds['quality'] == 3 ]\n",
    "q9 = vinhos_ds[vinhos_ds['quality'] == 9 ]\n",
    "\n",
    "q5_over = q5.sample(2836, replace=True)\n",
    "q7_over = q7.sample(2836, replace=True)\n",
    "q4_over = q4.sample(2836, replace=True)\n",
    "q8_over = q8.sample(2836, replace=True)\n",
    "q3_over = q3.sample(2836, replace=True)\n",
    "q9_over = q9.sample(2836, replace=True)\n",
    "\n",
    "new_vinhos_ds = pd.concat([q6,q5_over, q7_over, q4_over,q8_over,q3_over,q9_over], axis=0)\n",
    "\n",
    "#Dados agora balanceados\n",
    "print(new_vinhos_ds.quality.value_counts())\n",
    "\n",
    "X = new_vinhos_ds.drop('quality', axis=1)\n",
    "y = new_vinhos_ds['quality']\n",
    "#Dividinho em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Aplicando no modelo\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "#Verificando os resultados\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"É possível observar que todas as métricas de avalização melhoraram após a aplicação do oversampling\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Característica  Importância\n",
      "7                density     0.113123\n",
      "10               alcohol     0.111012\n",
      "4              chlorides     0.103611\n",
      "0          fixed acidity     0.095538\n",
      "5    free sulfur dioxide     0.095384\n",
      "1       volatile acidity     0.090553\n",
      "8                     pH     0.087964\n",
      "6   total sulfur dioxide     0.080503\n",
      "3         residual sugar     0.077917\n",
      "2            citric acid     0.071682\n",
      "9              sulphates     0.066842\n",
      "11           wine_is_red     0.005872\n"
     ]
    }
   ],
   "source": [
    "# Obtendo as características mais importantes do modelo\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Criando um DataFrame para visualizar a importância das características\n",
    "feature_importance_df = pd.DataFrame({'Característica': X.columns, 'Importância': feature_importance})\n",
    "\n",
    "# Ordenando as características pela importância em ordem decrescente\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importância', ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality\n",
      "6    2836\n",
      "5    2836\n",
      "7    2836\n",
      "4    2836\n",
      "8    2836\n",
      "3    2836\n",
      "9    2836\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      1.00      1.00       598\n",
      "           4       0.98      1.00      0.99       574\n",
      "           5       0.88      0.89      0.88       574\n",
      "           6       0.84      0.77      0.81       544\n",
      "           7       0.91      0.96      0.93       578\n",
      "           8       1.00      1.00      1.00       576\n",
      "           9       1.00      1.00      1.00       527\n",
      "\n",
      "    accuracy                           0.95      3971\n",
      "   macro avg       0.94      0.94      0.94      3971\n",
      "weighted avg       0.94      0.95      0.94      3971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ensemble learning Stacking\n",
    "\n",
    "vinhos_ds  = pd.read_csv(\"./winequality.csv\") #Lendo o arquivo para garantir que não tenha nenhuma alteração\n",
    "\n",
    "#Aplicando o oversample\n",
    "q6 = vinhos_ds[vinhos_ds['quality'] == 6 ]\n",
    "q5 = vinhos_ds[vinhos_ds['quality'] == 5 ]\n",
    "q7 = vinhos_ds[vinhos_ds['quality'] == 7 ]\n",
    "q4 = vinhos_ds[vinhos_ds['quality'] == 4 ]\n",
    "q8 = vinhos_ds[vinhos_ds['quality'] == 8 ]\n",
    "q3 = vinhos_ds[vinhos_ds['quality'] == 3 ]\n",
    "q9 = vinhos_ds[vinhos_ds['quality'] == 9 ]\n",
    "\n",
    "q5_over = q5.sample(2836, replace=True)\n",
    "q7_over = q7.sample(2836, replace=True)\n",
    "q4_over = q4.sample(2836, replace=True)\n",
    "q8_over = q8.sample(2836, replace=True)\n",
    "q3_over = q3.sample(2836, replace=True)\n",
    "q9_over = q9.sample(2836, replace=True)\n",
    "\n",
    "new_vinhos_ds = pd.concat([q6,q5_over, q7_over, q4_over,q8_over,q3_over,q9_over], axis=0)\n",
    "\n",
    "#Dados agora balanceados\n",
    "print(new_vinhos_ds.quality.value_counts())\n",
    "\n",
    "X = new_vinhos_ds.drop('quality', axis=1)\n",
    "y = new_vinhos_ds['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Modelos de base\n",
    "model1 = LinearRegression()\n",
    "model2 = RandomForestClassifier(random_state=42)\n",
    "model3 = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Treinar modelos de base nos dados de treinamento\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "pred1 = model1.predict(X_test)\n",
    "pred2 = model2.predict(X_test)\n",
    "pred3 = model3.predict(X_test)\n",
    "\n",
    "# Criar um conjunto de dados com as previsões dos modelos de base como features\n",
    "meta_features = np.column_stack((pred1, pred2, pred3))\n",
    "\n",
    "# Modelo meta (usando Regressão Linear) para combinar as previsões dos modelos de base\n",
    "meta_model = KNeighborsClassifier()\n",
    "meta_model.fit(meta_features, y_test)\n",
    "\n",
    "# Fazer previsões finais usando o modelo meta\n",
    "final_pred = meta_model.predict(meta_features)\n",
    "\n",
    "print(classification_report(y_test, final_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
